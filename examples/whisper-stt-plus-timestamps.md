# Whisper STT + timestamps

'![](https://img.shields.io/badge/tag-OpenAI-lightgrey)'  
{'![](https://img.shields.io/badge/tag-audio-lightgrey)'  
'![](https://img.shields.io/badge/tag-visualization-lightgrey)'  
'![](https://img.shields.io/badge/tag-Speech_to_Text-lightgrey)'  
'![](https://img.shields.io/badge/tag-timestamping-lightgrey)'  
'![](https://img.shields.io/badge/tag-experimental-lightgrey)'}



https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb#scrollTo=T5Gs4qD52D46

use grad cam on output tokens to infer per-token timestamps

see also: https://github.com/dmarx/bench-warmers/blob/main/subtitles-to-storyboard.md
